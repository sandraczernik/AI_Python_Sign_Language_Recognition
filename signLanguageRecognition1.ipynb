{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Accessing webcam using open cv and using the mediapipe import to apply detections to the camera.\n",
    "- OxFF creates a bit mask which sets the 24 bits on the left to zero\n",
    "- waitkey(1) returns a 32-bit integer\n",
    "- Since ord() returns a value between 255 and 0, a mask can be applies which checks if the corresponding key has been pressed, this is possible because of the limited character set on a keyboard\n",
    "- Converting image from BGR to RGB, then set it to unwritable to save on memory. The detection is then made, and the image is converted is set back to writeable and set back to BGR from RGB. Essentially cv2 will reads the webcam feed as BGR, but mediapipe will read it as RGB. \n",
    "\n",
    "https://google.github.io/mediapipe/solutions/holistic.html has been used to retrieve important functions for the mediapipe solution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "start = time.time()\n",
    "# importing dependencies\n",
    "# numpy for arrays and for structuring of datasets\n",
    "import numpy as np\n",
    "# os for file paths\n",
    "import os \n",
    "# importing open cv\n",
    "import cv2\n",
    "# time to give user time to get into position before using sign language\n",
    "import time\n",
    "# for visualising images\n",
    "from matplotlib import pyplot as plt\n",
    "# for hand detection\n",
    "import mediapipe as mp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drawing utilities set to draw detections and holostic model variable set to create detections\n",
    "mp_drawingUtilities = mp.solutions.drawing_utils\n",
    "mp_holosticModel = mp.solutions.holistic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# defining new function\n",
    "def mediapipe_detection(webcamImage,holosticModel):\n",
    "    # converting webcam image from BGR to RGB\n",
    "    webcamImage = cv2.cvtColor(webcamImage, cv2.COLOR_BGR2RGB)\n",
    "    # setting writeable to false to save on memory and then processing the image\n",
    "    webcamImage.flags.writeable = False\n",
    "    # making prediction from webcam image(cv2) and creating a result variable\n",
    "    preditctionResults = holosticModel.process(webcamImage)\n",
    "    webcamImage.flags.writeable = True\n",
    "    # converting webcam image back to BGR from RGB\n",
    "    webcamImage = cv2.cvtColor(webcamImage, cv2.COLOR_RGB2BGR)\n",
    "    # return image and prediction results from webcam for the loop\n",
    "    return webcamImage, preditctionResults"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define new function, grab image and results from mediapipe model\n",
    "def draw_styled_landmarks(webcamImage,preditctionResults):\n",
    "# drawing utilities used to draw landmarks using mediapipe to show landmarks and connections \n",
    "    # drawing right hand landmarks and hand connections\n",
    "    mp_drawingUtilities.draw_landmarks(webcamImage,preditctionResults.right_hand_landmarks, mp_holosticModel.HAND_CONNECTIONS, \n",
    "                                        mp_drawingUtilities.DrawingSpec(thickness=1, circle_radius=3, color=(0,0,255)),\n",
    "                                        mp_drawingUtilities.DrawingSpec(thickness=1, circle_radius=2, color=(0,0,255))\n",
    "                                        )                                     \n",
    "    # drawing left hand landmarks and hand connections\n",
    "    mp_drawingUtilities.draw_landmarks(webcamImage,preditctionResults.left_hand_landmarks, mp_holosticModel.HAND_CONNECTIONS,\n",
    "                                        mp_drawingUtilities.DrawingSpec(thickness=1, circle_radius=3, color=(0,255,0)),\n",
    "                                        mp_drawingUtilities.DrawingSpec(thickness=1, circle_radius=2, color=(0,255,0))\n",
    "                                        )\n",
    "    # drawing face landmarks and face connections\n",
    "    mp_drawingUtilities.draw_landmarks(webcamImage,preditctionResults.face_landmarks, mp_holosticModel.FACEMESH_CONTOURS,\n",
    "                                        mp_drawingUtilities.DrawingSpec(thickness=1, circle_radius=1, color=(255,0,0)),\n",
    "                                        mp_drawingUtilities.DrawingSpec(thickness=1, circle_radius=1, color=(255,0,0))\n",
    "                                        )\n",
    "    # drawing pose landmarks and pose connections\n",
    "    mp_drawingUtilities.draw_landmarks(webcamImage,preditctionResults.pose_landmarks, mp_holosticModel.POSE_CONNECTIONS,\n",
    "                                        mp_drawingUtilities.DrawingSpec(thickness=2, circle_radius=1, color=(102,0,102)),\n",
    "                                        mp_drawingUtilities.DrawingSpec(thickness=2, circle_radius=1, color=(102,0,102))\n",
    "                                        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- value - min_detection_confidence= for the detection to be confisdered successful\n",
    "- value - min_tracking_confidence= can be increased for increased robustness, but it creates more latency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set webcam as a video capture, set to device value 0\n",
    "webcamCapture = cv2.VideoCapture(0)\n",
    "# creating initial detection confidence and tracking confidence values in the mediapipe model and then running the while loop which starts the webcam feed\n",
    "with mp_holosticModel.Holistic(min_detection_confidence=0.5, min_tracking_confidence=0.7) as holistic:\n",
    "        # read webcam caputre\n",
    "        ret, webcamFrame = webcamCapture.read()\n",
    "        # unpacking results from above def function to make detections\n",
    "        webcamImage, preditctionResults = mediapipe_detection(webcamFrame, holistic)\n",
    "        # drawing landmarks on live webcam feed\n",
    "        draw_styled_landmarks(webcamImage, preditctionResults)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "#####left hand landmarks array. If left_hand_landmarks are not present, an empty array of 21*3 zeroes will be put in place instead. The reason for this number is because there are 21 connections in the \n",
    "#left and right hand, and each of those have 3 coordinates, meaning there will always be 63 elements in the array for the left and right hand respectively.#####\n",
    "leftHandLandmarks = np.array([[landmarkResults.x, landmarkResults.y, landmarkResults.z] for landmarkResults in preditctionResults.left_hand_landmarks.landmark]).flatten() if preditctionResults.left_hand_landmarks else np.zeros(21*3)\n",
    "rightHandLandmarks = np.array([[landmarkResults.x, landmarkResults.y, landmarkResults.z] for landmarkResults in preditctionResults.right_hand_landmarks.landmark]).flatten() if preditctionResults.right_hand_landmarks else np.zeros(21*3)\n",
    "#####face landmarks\n",
    "faceLandmarks = np.array([[landmarkResults.x, landmarkResults.y, landmarkResults.z] for landmarkResults in preditctionResults.face_landmarks.landmark]).flatten() if preditctionResults.face_landmarks else np.zeros(468*3)\n",
    "#####pose landmarks = connections between joints, such as hand and arm, or arm and shoulder#####\n",
    "poseLandmarks = np.array([[landmarkResults.x, landmarkResults.y, landmarkResults.z, landmarkResults.visibility] for landmarkResults in preditctionResults.pose_landmarks.landmark]).flatten() if preditctionResults.pose_landmarks else np.zeros(33*4)\n",
    "#####printing all landmark results for testing\n",
    "#####len() function allowed for the creation of a specific amounts of zeros in the array which can be seen at the end of each array#####\n",
    "#leftHandLandmarks\n",
    "#rightHandLandmarks\n",
    "#faceLandmarks\n",
    "#poseLandmarks\n",
    "\n",
    "# creating function to extract all landmarks\n",
    "def extract_Landmark_Keypoints(preditctionResults):\n",
    "    leftHandLandmarks = np.array([[landmarkResults.x, landmarkResults.y, landmarkResults.z] for landmarkResults in preditctionResults.left_hand_landmarks.landmark]).flatten() if preditctionResults.left_hand_landmarks else np.zeros(21*3)\n",
    "    rightHandLandmarks = np.array([[landmarkResults.x, landmarkResults.y, landmarkResults.z] for landmarkResults in preditctionResults.right_hand_landmarks.landmark]).flatten() if preditctionResults.right_hand_landmarks else np.zeros(21*3)\n",
    "    faceLandmarks = np.array([[landmarkResults.x, landmarkResults.y, landmarkResults.z] for landmarkResults in preditctionResults.face_landmarks.landmark]).flatten() if preditctionResults.face_landmarks else np.zeros(468*3)\n",
    "    poseLandmarks = np.array([[landmarkResults.x, landmarkResults.y, landmarkResults.z, landmarkResults.visibility] for landmarkResults in preditctionResults.pose_landmarks.landmark]).flatten() if preditctionResults.pose_landmarks else np.zeros(33*4)\n",
    "# concatenating all keypoints\n",
    "    return np.concatenate([leftHandLandmarks, rightHandLandmarks, faceLandmarks, poseLandmarks])\n",
    "# dispalying amount of elements in array\n",
    "extract_Landmark_Keypoints(preditctionResults).shape\n",
    "# display first 20 elements from array\n",
    "##extract_Landmark_Keypoints(preditctionResults)[:20]\n",
    "resultTest = extract_Landmark_Keypoints(preditctionResults)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting up folders for exported data for collection\n",
    "DATA_PATH = os.path.join('ASL-Phrases')\n",
    "\n",
    "# actions that will be detected\n",
    "gestureActions = np.array(['hello', 'good morning', 'welcome', 'thankyou'])\n",
    "# amount of videos taken for data collection, 40 videos worth of data and videos will be 30 frames long\n",
    "sequence_Number = 40\n",
    "sequence_Length = 30\n",
    "# creating folders to store data in\n",
    "# for each action in gesture action array, create a folder with the name of each element in the gestureActions array, then create a subfolder up to number 39 (sequence_Number = 40)\n",
    "for eachAction in gestureActions:\n",
    "    # looping through 40 different videos\n",
    "    for gestureSequence in range(sequence_Number):\n",
    "        try:\n",
    "            # if loop is successfull, create a folder for each gesture sequence, if folder exists, pass (folders are provided as they custom data has been gathered for this project)\n",
    "            os.makedirs(os.path.join(DATA_PATH, eachAction, str(gestureSequence)))\n",
    "        except:\n",
    "            pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### THIS IS HOW DATA WAS COLLECTED, LEAVE COMMENTED OUT OR IT WILL OVERWRITE THE EXISTING DATASET #####\n",
    "# set webcam as a video capture, set to device value 0\n",
    "# webcamCapture = cv2.VideoCapture(0)\n",
    "#  # creating initial detection confidence and tracking confidence values in the mediapipe model and then running the while loop which starts the webcam feed\n",
    "# with mp_holosticModel.Holistic(min_detection_confidence=0.5, min_tracking_confidence=0.7) as holistic:\n",
    "#      # loop through each action\n",
    "#      for eachAction in gestureActions:\n",
    "#          # loop through each gesture sequence\n",
    "#          for gestureSequence in range(sequence_Number):\n",
    "#             # loop through each video\n",
    "#             for frameNumber in range(sequence_Length):\n",
    "                 # read webcam caputre\n",
    "                # ret, webcamFrame = webcamCapture.read()\n",
    "                #  # unpacking results from above def function to make detections\n",
    "                # webcamImage, preditctionResults = mediapipe_detection(webcamFrame, holistic)\n",
    "                # print(preditctionResults)\n",
    "                #  # drawing landmarks on live webcam feed\n",
    "                # draw_styled_landmarks(webcamImage, preditctionResults)\n",
    "\n",
    "                #  # creating breaks in between each video capture so the hand can be moved easily for each video taken\n",
    "                # if frameNumber == 0:\n",
    "                #     cv2.putText(webcamImage, 'COLLECTING DATA', (120,200),\n",
    "                #                  cv2.FONT_HERSHEY_PLAIN, 1, (0, 0, 255), 4, cv2.LINE_AA)\n",
    "                #     cv2.putText(webcamImage, 'COLLECTING FRAMES FOR {} VIDEO NUMBER {}'.format(eachAction, gestureSequence), (15,12),\n",
    "                #                  cv2.FONT_HERSHEY_PLAIN, 0.5, (0, 0, 255), 1, cv2.LINE_AA)\n",
    "                #     cv2.waitKey(3000)  \n",
    "                    \n",
    "                # else:\n",
    "                #      cv2.putText(webcamImage, 'COLLECTING FRAMES FOR {} VIDEO NUMBER {}'.format(eachAction, gestureSequence), (15,12),\n",
    "                #      cv2.FONT_HERSHEY_PLAIN, 0.5, (0, 0, 255), 1, cv2.LINE_AA)       \n",
    "                             \n",
    "        \n",
    "                #  # extracting keypoints and saving them into folders\n",
    "                # savedKeypoints = extract_Landmark_Keypoints(preditctionResults)\n",
    "                # npy_path = os.path.join(DATA_PATH, eachAction, str(gestureSequence), str(frameNumber))\n",
    "                # np.save(npy_path, savedKeypoints)\n",
    "\n",
    "                 # show webcam on screen\n",
    "#                 cv2.imshow('Webcam feed', webcamImage)\n",
    "                \n",
    "            \n",
    "#                  # if n is pressed, the loop ends\n",
    "#                 if cv2.waitKey(10) & 0xFF == ord('n'):\n",
    "#                     break\n",
    "#         # closes video file or capturing device\n",
    "# webcamCapture.release()\n",
    "#         # destory the specific window created with imshow()\n",
    "# cv2.destroyWindow('Webcam feed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(40, 30, 1662)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# importing more dependencies to preprocess data, create labels and create features\n",
    "# convert data\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "# to create and seperate training and testing data\n",
    "from sklearn.model_selection import train_test_split\n",
    "# creating label map and looping through each gesture action (words)\n",
    "labelMap = {label:number for number, label in enumerate(gestureActions)}\n",
    "\n",
    "featureSequences, labels = [], []\n",
    "for eachAction in gestureActions:\n",
    "        # loop through each gesture sequence\n",
    "        for gestureSequence in range(sequence_Number):\n",
    "            window = []\n",
    "            # loop through each video\n",
    "            for frameNumber in range(sequence_Length):\n",
    "                labelMapResult = np.load(os.path.join(DATA_PATH, eachAction, str(gestureSequence), \"{}.npy\".format(frameNumber)))\n",
    "                # once loop has finished going through one video, append it to the labelMapresult and start again for the next video\n",
    "                window.append(labelMapResult)\n",
    "            # grabbing video and appending it to featureSequences array\n",
    "            featureSequences.append(window)\n",
    "            # appending label for each action\n",
    "            labels.append(labelMap[eachAction])\n",
    "\n",
    "##np.array(featureSequences).shape\n",
    "##featureSequences\n",
    "\n",
    "X = np.array(featureSequences)\n",
    "##X.shape\n",
    "# creating array which shows 0 or 1 and represents the letters of the alphabet. [1, 0, 0, 0] == a, [0, 1, 0, 0] == b and so on...\n",
    "y = to_categorical(labels).astype(int)\n",
    "##y\n",
    "# unpacking train test split function, test size = 0.15 (10%)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25)\n",
    "X_test.shape\n",
    "#X_train.shape\n",
    "#y_test.shape\n",
    "#y_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Building and training neural network LSTM using keras and tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import sequential model, LSTM, Dense and the tensor board\n",
    "# to create sequential neural network\n",
    "from tensorflow.keras.models import Sequential\n",
    "# importing LSTM = temporal component and allows to perform action detection, Dense = fully connected layer\n",
    "from tensorflow.keras.layers import LSTM, Dense\n",
    "# logginig in tensor board to trace model as its being trained\n",
    "from tensorflow.keras.callbacks import TensorBoard\n",
    "from keras.callbacks import TensorBoard\n",
    "# tensor board = web app provided with tensorflow package, allows for logging and monitoring neutral network training and the accuracy\n",
    "log_dir = os.path.join('TensorBoardLogs')\n",
    "tb_callback = TensorBoard(log_dir=log_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "why lstm and mediapipe\n",
    "- usually cnn and then lstm is used (requires alot of pretrained data for accuracy)\n",
    "- lstm and mediapipe allows for less pretrained data needed and is more accurate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm_6 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_7 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_8 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
     ]
    }
   ],
   "source": [
    "sequentialModel = Sequential()\n",
    "# adding 3 sets of LSTM layers\n",
    "# 3 keyword argument, 1 positional argument, 64 LSTM units, return true if u want to stack sequences, activation relu, but can be changed. \n",
    "# Input shape is 30 frames by 1662 which is essentially the X.shape elements\n",
    "sequentialModel.add(LSTM(64, return_sequences=True, activation='relu' , input_shape=(30,1662)))\n",
    "# 128 LSTM units, return sequence for next 64 LSTM units\n",
    "sequentialModel.add(LSTM(128, return_sequences=True, activation='relu'))\n",
    "# 64 LSTM units, return sequences to false because next layer is a dense layer, and the sequences are not needed for that\n",
    "sequentialModel.add(LSTM(64, return_sequences=False, activation='relu'))\n",
    "# using fully connected layers, 64 dense units \n",
    "sequentialModel.add(Dense(64, activation='relu'))\n",
    "# 32 dense units\n",
    "sequentialModel.add(Dense(32, activation='relu'))\n",
    "# return output of model    , softmax returns values with a probability of 0 to 1 (array of probability that adds up to one)\n",
    "sequentialModel.add(Dense(gestureActions.shape[0], activation='softmax'))\n",
    "# compiling model\n",
    "# optimizer can be changed to different values such as (SGD, RMSprop, Adam, Adadelta, Adagrad, Adamax, Nadam, Ftrl)\n",
    "# loss cannot be changed, this is for multi-class classification model, however if the classification was binary, binary_crossentropy would be used, regression would be different too\n",
    "# metrics is optional, but useful if you want to track your accuracy as you train\n",
    "sequentialModel.compile(optimizer='Adam', loss='categorical_crossentropy', metrics=['categorical_accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_6 (LSTM)                (None, 30, 64)            442112    \n",
      "_________________________________________________________________\n",
      "lstm_7 (LSTM)                (None, 30, 128)           98816     \n",
      "_________________________________________________________________\n",
      "lstm_8 (LSTM)                (None, 64)                49408     \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 64)                4160      \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 32)                2080      \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 4)                 132       \n",
      "=================================================================\n",
      "Total params: 596,708\n",
      "Trainable params: 596,708\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "# # MODEL FITTING - NOT NECESSARY AS MODEL WILL BE IMPORTED WITH THE WEIGHTS FUNCTION\n",
    "#  #can stop training earlier if accuracy is good and loss has stopped decreasing substantially \n",
    "#  #pass through X training datam y training data, 1000 passes = epochs, \n",
    "#  #data will fit into memory, so a data generator does not need to be build for a pipeline of data\n",
    "# tb_callback = keras.callbacks.EarlyStopping(monitor='val_loss', \n",
    "#                                  min_delta=0, \n",
    "#                                  patience=0, \n",
    "#                                  verbose=0, \n",
    "#                                  mode='auto', \n",
    "#                                  baseline=None, \n",
    "#                                  restore_best_weights=False)\n",
    "# sequentialModel.fit(X_train, y_train, epochs=40, callbacks=[tb_callback])\n",
    "# summary of sequential model\n",
    "sequentialModel.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "####making model predictions\n",
    "# res = sequentialModel.predict(X_test)\n",
    "# #np.sum(res[0])\n",
    "# ARRAY [1] IN RED SHOULD RESULT IN THE SAME AS ARRAY [1] IN Y_TEST\n",
    "#gestureActions[np.argmax(res[1])]\n",
    "#gestureActions[np.argmax(y_test[1])]\n",
    "# saving model and weights\n",
    "# sequentialModel.save('phrases25.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading weights for future use\n",
    "sequentialModel.load_weights('phrases25.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.925"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# evaluation using confusion matrix and accuracy\n",
    "# importing matrix from sklearn module\n",
    "#multi lasbel confusion matrix, evluating true postitive and true negative, false positive and false negative\n",
    "from sklearn.metrics import multilabel_confusion_matrix, accuracy_score\n",
    "# predictions\n",
    "yProb = sequentialModel.predict(X_test)\n",
    "#yhat = sequentialModel.predict(X_train)\n",
    "# evaluating\n",
    "ytrue = np.argmax(y_test, axis=1).tolist()\n",
    "yProb = np.argmax(yProb, axis=1).tolist()\n",
    "\n",
    "multilabel_confusion_matrix(ytrue, yProb)\n",
    "accuracy_score(ytrue, yProb)\n",
    "##TESTING\n",
    "#np.expand_dims(X_test[0], axis=0).shape\n",
    "# sequentialModel.predict(np.expand_dims(X_test[0], axis=0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # USED DURING TESTING TO QUICKLY CLOSE CAMERA WINDOW IF AN ERROR OCCURED \n",
    "# webcamCapture.release()\n",
    "# cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Final product including GUI - the use of tkinter was considered however was not necessary as cv2 provided all neccessary GUI elements such as rectangles or texts to display predictions to user."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Application....\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "can only concatenate str (not \"float\") to str",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\Sandra\\Documents\\GitHub\\AI_Python_Sign_Language_Recognition\\signLanguageRecognition1.ipynb Cell 22'\u001b[0m in \u001b[0;36m<cell line: 95>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Sandra/Documents/GitHub/AI_Python_Sign_Language_Recognition/signLanguageRecognition1.ipynb#ch0000021?line=84'>85</a>\u001b[0m     \u001b[39m# im = Image.fromarray(webcamImage)\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Sandra/Documents/GitHub/AI_Python_Sign_Language_Recognition/signLanguageRecognition1.ipynb#ch0000021?line=85'>86</a>\u001b[0m     \u001b[39m# imgtk = ImageTk.PhotoImage(image=im)\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Sandra/Documents/GitHub/AI_Python_Sign_Language_Recognition/signLanguageRecognition1.ipynb#ch0000021?line=86'>87</a>\u001b[0m     \u001b[39m# Label(win,image=imgtk).place(x=0, y=0, anchor=\"w\")\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Sandra/Documents/GitHub/AI_Python_Sign_Language_Recognition/signLanguageRecognition1.ipynb#ch0000021?line=91'>92</a>\u001b[0m     \u001b[39m# endButton.pack()\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Sandra/Documents/GitHub/AI_Python_Sign_Language_Recognition/signLanguageRecognition1.ipynb#ch0000021?line=92'>93</a>\u001b[0m     \u001b[39m# win.mainloop()  \u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Sandra/Documents/GitHub/AI_Python_Sign_Language_Recognition/signLanguageRecognition1.ipynb#ch0000021?line=93'>94</a>\u001b[0m end \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime()\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/Sandra/Documents/GitHub/AI_Python_Sign_Language_Recognition/signLanguageRecognition1.ipynb#ch0000021?line=94'>95</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39;49m\u001b[39mruntime: \u001b[39;49m\u001b[39m\"\u001b[39;49m \u001b[39m+\u001b[39;49m end \u001b[39m-\u001b[39m start)\n",
      "\u001b[1;31mTypeError\u001b[0m: can only concatenate str (not \"float\") to str"
     ]
    }
   ],
   "source": [
    "#GUI\n",
    "from tkinter import *\n",
    "import tkinter as tk\n",
    "import numpy as np\n",
    "from PIL import Image, ImageTk\n",
    "class VideoCallSignLanguageDetection:\n",
    "    def show_frames(webcamImage):\n",
    "        \n",
    "        # real time detection\n",
    "        # detection variables\n",
    "        sequence = []\n",
    "        words = []\n",
    "        threshold = 0.6\n",
    "\n",
    "        # set webcam as a video capture, set to device value 0\n",
    "        webcamCapture = cv2.VideoCapture(0)\n",
    "        # creating initial detection confidence and tracking confidence values in the mediapipe model and then running the while loop which starts the webcam feed\n",
    "        with mp_holosticModel.Holistic(min_detection_confidence=0.5, min_tracking_confidence=0.7) as holistic:\n",
    "            # while webcam is open, loop every frame\n",
    "            while webcamCapture.isOpened():\n",
    "                # read webcam caputre\n",
    "                ret, webcamFrame = webcamCapture.read()\n",
    "                # unpacking results from above def function to make detections\n",
    "                webcamImage, preditctionResults = mediapipe_detection(webcamFrame, holistic)\n",
    "                #ENABLE PRINT BELOW TO SEE PREDICTION RESULTS IN COMMAND LINE\n",
    "                #print(preditctionResults)\n",
    "\n",
    "                # drawing landmarks on live webcam feed\n",
    "                #draw_styled_landmarks(webcamImage, preditctionResults)\n",
    "\n",
    "                # logic for predicting\n",
    "                # grabbing keypoints from earlier function and appending them into sequence array\n",
    "                keypoints = extract_Landmark_Keypoints(preditctionResults)\n",
    "                sequence.insert(0, keypoints)\n",
    "                # grab last 40 frames for prediction\n",
    "                sequence = sequence[:30]\n",
    "\n",
    "                #if sequence is == 40, then a prediction will be made\n",
    "                if len(sequence) == 30:\n",
    "                    res = sequentialModel.predict(np.expand_dims(sequence, axis=0))[0]\n",
    "                   # print(gestureActions[np.argmax(res)])\n",
    "\n",
    "            #     ## visualisation logic\n",
    "            #     # check if result is above threshold\n",
    "            \n",
    "                    if res[np.argmax(res)] > threshold:\n",
    "                        if len(words) > 0:\n",
    "                            if gestureActions[np.argmax(res)] != words[-1]:\n",
    "                                words.append(gestureActions[np.argmax(res)])\n",
    "                        else:\n",
    "                                words.append(gestureActions[np.argmax(res)])\n",
    "                    if len(words) > 1:\n",
    "                        words = words[-1:]\n",
    "\n",
    "\n",
    "                cv2.rectangle(webcamImage, (0, 800), (640,420), (128, 128, 128), -1)\n",
    "                cv2.putText(webcamImage, ' '.join(words), (3,450), \n",
    "                                cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 0), 2, cv2.LINE_AA)\n",
    "                cv2.putText(\n",
    "                    img = webcamImage,\n",
    "                    text = 'Press Q to close',\n",
    "                    org = (480,20),\n",
    "                    fontFace= cv2.FONT_HERSHEY_SIMPLEX,\n",
    "                    fontScale= 0.5, \n",
    "                    color=(0, 0, 0), \n",
    "                    thickness= 1, \n",
    "                    lineType= cv2.LINE_AA)\n",
    "                cv2.copyMakeBorder(webcamImage, 10, 10, 10, 10, cv2.BORDER_CONSTANT, None, value = 0)\n",
    "                # show webcam on screen\n",
    "                cv2.imshow('Sign Language Detector for Video Calls with Live Feedback', webcamImage)\n",
    "                # if n is pressed, the loop ends\n",
    "          \n",
    "\n",
    "                if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "                    break\n",
    "                # closes video file or capturing device\n",
    "        webcamCapture.release()\n",
    "        # destory the specific window created with imshow()\n",
    "        cv2.destroyWindow('Sign Language Detector for Video Calls with Live Feedback')\n",
    "\n",
    "    # win = tk.Tk()\n",
    "    # win.title('Sign Language Detector for Video Calls with Live Feedback')\n",
    "    print(\"Starting Application....\")\n",
    "    show_frames(webcamImage)\n",
    "    # im = Image.fromarray(webcamImage)\n",
    "    # imgtk = ImageTk.PhotoImage(image=im)\n",
    "    # Label(win,image=imgtk).place(x=0, y=0, anchor=\"w\")\n",
    "    # #win.geometry(\"720x480\")\n",
    "    # startButton = Button(win, text='Begin Detecting', width=25, command=show_frames(webcamCapture))\n",
    "    # endButton = Button(win, text='Quit', width=25, command=win.destroy)\n",
    "    # startButton.pack()\n",
    "    # endButton.pack()\n",
    "    # win.mainloop()  \n",
    "end = time.time()\n",
    "print(\"RUNTIME: \")\n",
    "print(end - start)\n",
    "print(\"RUNTIME END\")\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "3e400a43e9a0a6684b7988f8fb7e1576167ce7450345cb820b61e2e2b05916c5"
  },
  "kernelspec": {
   "display_name": "Python 3.9.9 64-bit (windows store)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
